{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI agent prompt design for llama_cpp_canister"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify we're in the Conda environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import base64\n",
    "import io\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import pprint\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import jupyter_black\n",
    "import textwrap\n",
    "\n",
    "# Activate the jupyter_black extension, which reformats code cells with black\n",
    "# https://github.com/n8henrie/jupyter-black\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run llama.cpp\n",
    "\n",
    "\n",
    "def run_llama_cpp(\n",
    "    llama_cli_path,\n",
    "    model,\n",
    "    prompt,\n",
    "    num_tokens,\n",
    "    seed,\n",
    "    temp,\n",
    "    # top_k,\n",
    "    # top_p,\n",
    "    # min_p,\n",
    "    # tfs,\n",
    "    # typical,\n",
    "    # mirostat,\n",
    "    # mirostat_lr,\n",
    "    # mirostat_ent,\n",
    "    repeat_penalty,\n",
    "):\n",
    "\n",
    "    command = [\n",
    "        llama_cli_path,\n",
    "        \"-m\",\n",
    "        model,\n",
    "        \"--no-warmup\",  # needed when running from CLI. Is default for llama_cpp_canister\n",
    "        \"-no-cnv\",  # needed when running from CLI. Is default for llama_cpp_canister\n",
    "        # \"--simple-io\",\n",
    "        # \"--no-display-prompt\",  # only return the generated text, without special characters\n",
    "        \"-sp\",  # output special tokens\n",
    "        \"-n\",\n",
    "        f\"{num_tokens}\",\n",
    "        \"--seed\",\n",
    "        f\"{seed}\",\n",
    "        \"--temp\",\n",
    "        f\"{temp}\",\n",
    "        # \"--top-k\",\n",
    "        # f\"{top_k}\",\n",
    "        # \"--top-p\",\n",
    "        # f\"{top_p}\",\n",
    "        # \"--min-p\",\n",
    "        # f\"{min_p}\",\n",
    "        # \"--tfs\",\n",
    "        # f\"{tfs}\",\n",
    "        # \"--typical\",\n",
    "        # f\"{typical}\",\n",
    "        # \"--mirostat\",\n",
    "        # f\"{mirostat}\",\n",
    "        # \"--mirostat-lr\",\n",
    "        # f\"{mirostat_lr}\",\n",
    "        # \"--mirostat-ent\",\n",
    "        # f\"{mirostat_ent}\",\n",
    "        \"--repeat-penalty\",\n",
    "        f\"{repeat_penalty}\",\n",
    "        \"-p\",\n",
    "        prompt,\n",
    "    ]\n",
    "\n",
    "    # Print the command on a single line for terminal use, preserving \\n\n",
    "    print(\n",
    "        \"\\nCommand:\\n\",\n",
    "        f\"{llama_cli_path} -m {model} --no-warmup -no-cnv -sp -n {num_tokens} --seed {seed} --temp {temp} --repeat-penalty {repeat_penalty}  -p '{prompt}'\".replace(\n",
    "            \"\\n\", \"\\\\n\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    subprocess.run(command, text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################################################ #\n",
    "# Define where the llama-cli is located, relative to this notebook #\n",
    "# ################################################################ #\n",
    "\n",
    "# Using the latest version of llama.cpp\n",
    "# LLAMA_CLI_PATH = \"../../ggerganov_llama_latest.cpp/build/bin/llama-cli\"\n",
    "\n",
    "# lama.cpp git sha 615212 is used by icpp-pro 5.0.2 and later\n",
    "LLAMA_CLI_PATH = \"../../ggerganov_llama_615212.cpp/build/bin/llama-cli\"\n",
    "\n",
    "# lama.cpp git sha b841d0 is used by icpp-pro 5.0.1 and earlier\n",
    "# LLAMA_CLI_PATH = \"../../ggerganov_llama_b841d0.cpp/llama-cli\" # Current llama_cpp_canister version\n",
    "\n",
    "\n",
    "\n",
    "# ####################################################################### #\n",
    "# Define the MODEL_TYPE and MODEL (location is relative to this notebook) #\n",
    "# ####################################################################### #\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# MODEL_TYPE = \"gpt2\"\n",
    "\n",
    "# https://huggingface.co/tensorblock/gpt2-GGUF (124M)\n",
    "# MODEL = \"../models/tensorblock/gpt2-GGUF/gpt2-Q8_0.gguf\"\n",
    "\n",
    "# https://huggingface.co/tensorblock/gpt2-medium-GGUF (355M)\n",
    "# MODEL = \"../models/tensorblock/gpt2-medium-GGUF/gpt2-medium-Q8_0.gguf\"\n",
    "\n",
    "# https://huggingface.co/tensorblock/gpt2-large-GGUF (774M)\n",
    "# MODEL = \"../models/tensorblock/gpt2-medium-GGUF/gpt2-large-Q8_0.gguf\"\n",
    "\n",
    "# https://huggingface.co/PrunaAI/gpt2-GGUF-smashed (163M)\n",
    "# MODEL = \"../models/PrunaAI/gpt2-GGUF-smashed/gpt2.Q5_K_M.gguf\"\n",
    "# MODEL = \"../models/PrunaAI/gpt2-GGUF-smashed/gpt2.Q8_0.gguf\"\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# MODEL_TYPE = \"SmolLM2\"\n",
    "\n",
    "# https://huggingface.co/tensorblock/SmolLM2-135M-Instruct-GGUF\n",
    "# MODEL = (\n",
    "#     \"../models/tensorblock/SmolLM2-135M-Instruct-GGUF/SmolLM2-135M-Instruct-Q4_K_M.gguf\"\n",
    "# )\n",
    "# MODEL = \"../models/tensorblock/SmolLM2-135M-Instruct-GGUF/SmolLM2-135M-Instruct-Q8_0.gguf\"\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "MODEL_TYPE = \"Qwen\"\n",
    "\n",
    "# https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF\n",
    "MODEL = \"../models/Qwen/Qwen2.5-0.5B-Instruct-GGUF/qwen2.5-0.5b-instruct-q8_0.gguf\"\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# MODEL_TYPE = \"DeepSeek-R1-Distill-Qwen\"\n",
    "#\n",
    "# https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF\n",
    "# MODEL = \"../models/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.gguf\"\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# MODEL_TYPE = \"Llama-3.2\"\n",
    "#\n",
    "# https://huggingface.co/unsloth/Llama-3.2-1B-Instruct-GGUF\n",
    "# MODEL = \"../models/unsloth/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct-Q2_K.gguf\"\n",
    "\n",
    "# https://huggingface.co/unsloth/Llama-3.2-3B-Instruct-GGUF\n",
    "# MODEL = \"../models/unsloth/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct-Q2_K.gguf\"\n",
    "\n",
    "\n",
    "seed = 42\n",
    "num_tokens = 1024\n",
    "temp = 0.7\n",
    "# top_k = 50\n",
    "# top_p = 0.95\n",
    "# min_p = 0.05\n",
    "# tfs = 0.9\n",
    "# typical = 0.9\n",
    "# mirostat = 2\n",
    "# mirostat_lr = 0.1\n",
    "# mirostat_ent = 5.0\n",
    "repeat_penalty = 1.1\n",
    "\n",
    "prompt = \"\"\n",
    "# question = \"What is the Internet Computer Protocol?\"\n",
    "question = \"What is a blockchain?\"\n",
    "if MODEL_TYPE == \"gpt2\":\n",
    "    prompt = f\"{question}.\"\n",
    "elif MODEL_TYPE in [\"Qwen\", \"SmolLM2\", \"DeepSeek-R1-Distill-Qwen\"]:\n",
    "    prompt = f\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "elif MODEL_TYPE == \"Llama-3.2\":\n",
    "    system_prompt = \"\"\n",
    "    prompt = f\"<|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "else:\n",
    "    print(f\"Model type {MODEL_TYPE} not recognized\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"\\nprompt:\\n\", textwrap.fill(prompt, width=80))\n",
    "\n",
    "run_llama_cpp(\n",
    "    LLAMA_CLI_PATH,\n",
    "    MODEL,\n",
    "    prompt,\n",
    "    num_tokens,\n",
    "    seed,\n",
    "    temp,\n",
    "    # top_k,\n",
    "    # top_p,\n",
    "    # min_p,\n",
    "    # tfs,\n",
    "    # typical,\n",
    "    # mirostat,\n",
    "    # mirostat_lr,\n",
    "    # mirostat_ent,\n",
    "    repeat_penalty,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_cpp_canister",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
