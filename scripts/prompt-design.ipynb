{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# AI agent prompt design for llama_cpp_canister"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Setup"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Verify we're in the Conda environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "\n",
                "print(sys.executable)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Import python packages"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import json\n",
                "import base64\n",
                "import io\n",
                "from dotenv import load_dotenv\n",
                "import requests\n",
                "import pprint\n",
                "from pathlib import Path\n",
                "import subprocess\n",
                "import jupyter_black\n",
                "import textwrap\n",
                "\n",
                "# Activate the jupyter_black extension, which reformats code cells with black\n",
                "# https://github.com/n8henrie/jupyter-black\n",
                "jupyter_black.load()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define where the llama-cli is located, relative to this notebook\n",
                "# LLAMA_CLI_PATH = \"../../ggerganov_llama_b841d0.cpp/llama-cli\" # Current llama_cpp_canister version\n",
                "LLAMA_CLI_PATH = \"../../ggerganov_llama_latest.cpp/build/bin/llama-cli\"\n",
                "\n",
                "# Select a model to use\n",
                "# MODEL = \"../models/Qwen/Qwen2.5-0.5B-Instruct-GGUF/qwen2.5-0.5b-instruct-q8_0.gguf\"\n",
                "# MODEL = \"../models/tensorblock/SmolLM2-135M-Instruct-GGUF/SmolLM2-135M-Instruct-Q8_0.gguf\"\n",
                "# MODEL = (\n",
                "#     \"../models/tensorblock/SmolLM2-135M-Instruct-GGUF/SmolLM2-135M-Instruct-Q4_K_M.gguf\"\n",
                "# )\n",
                "MODEL = \"../models/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.gguf\"\n",
                "# MODEL = \"../models/unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF/DeepSeek-R1-Distill-Qwen-7B-Q2_K.gguf\"\n",
                "\n",
                "\n",
                "def run_llama_cpp(\n",
                "    prompt,\n",
                "    num_tokens,\n",
                "    seed,\n",
                "    temp,\n",
                "    # top_k,\n",
                "    # top_p,\n",
                "    # min_p,\n",
                "    # tfs,\n",
                "    # typical,\n",
                "    # mirostat,\n",
                "    # mirostat_lr,\n",
                "    # mirostat_ent,\n",
                "):\n",
                "\n",
                "    command = [\n",
                "        LLAMA_CLI_PATH,\n",
                "        \"-m\",\n",
                "        MODEL,\n",
                "        \"--no-warmup\", # needed when running from CLI. Is default for llama_cpp_canister\n",
                "        \"-no-cnv\", # needed when running from CLI. Is default for llama_cpp_canister\n",
                "        # \"--simple-io\",\n",
                "        # \"--no-display-prompt\",  # only return the generated text, without special characters\n",
                "        \"-sp\",  # output special tokens\n",
                "        \"-n\",\n",
                "        f\"{num_tokens}\",\n",
                "        \"--seed\",\n",
                "        f\"{seed}\",\n",
                "        \"--temp\",\n",
                "        f\"{temp}\",\n",
                "        # \"--top-k\",\n",
                "        # f\"{top_k}\",\n",
                "        # \"--top-p\",\n",
                "        # f\"{top_p}\",\n",
                "        # \"--min-p\",\n",
                "        # f\"{min_p}\",\n",
                "        # \"--tfs\",\n",
                "        # f\"{tfs}\",\n",
                "        # \"--typical\",\n",
                "        # f\"{typical}\",\n",
                "        # \"--mirostat\",\n",
                "        # f\"{mirostat}\",\n",
                "        # \"--mirostat-lr\",\n",
                "        # f\"{mirostat_lr}\",\n",
                "        # \"--mirostat-ent\",\n",
                "        # f\"{mirostat_ent}\",\n",
                "        \"-p\",\n",
                "        prompt,\n",
                "    ]\n",
                "\n",
                "    # Print the command on a single line for terminal use, preserving \\n\n",
                "    print(\n",
                "        \"\\nCommand:\\n\",\n",
                "        f\"{LLAMA_CLI_PATH} -m {MODEL} --no-warmup -no-cnv -sp -n {num_tokens} --seed {seed} --temp {temp} -p '{prompt}'\".replace(\n",
                "            \"\\n\", \"\\\\n\"\n",
                "        ),\n",
                "    )\n",
                "\n",
                "    # Run the command and capture the output\n",
                "    result = subprocess.run(\n",
                "        command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True\n",
                "    )\n",
                "    output = result.stdout\n",
                "    return output\n",
                "\n",
                "\n",
                "seed = 42\n",
                "num_tokens = 1024\n",
                "temp = 0.7\n",
                "# top_k = 50\n",
                "# top_p = 0.95\n",
                "# min_p = 0.05\n",
                "# tfs = 0.9\n",
                "# typical = 0.9\n",
                "# mirostat = 2\n",
                "# mirostat_lr = 0.1\n",
                "# mirostat_ent = 5.0\n",
                "\n",
                "prompt = f\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nWhat is the Proof-of-AI-Work Protocol?<|im_end|>\\n<|im_start|>assistant\\n\"\n",
                "response = run_llama_cpp(\n",
                "    prompt,\n",
                "    num_tokens,\n",
                "    seed,\n",
                "    temp,\n",
                "    # top_k,\n",
                "    # top_p,\n",
                "    # min_p,\n",
                "    # tfs,\n",
                "    # typical,\n",
                "    # mirostat,\n",
                "    # mirostat_lr,\n",
                "    # mirostat_ent,\n",
                ")\n",
                "\n",
                "import textwrap\n",
                "\n",
                "print(\"\\nprompt:\\n\", textwrap.fill(prompt, width=80))\n",
                "print(\"\\nresponse:\\n\", textwrap.fill(response, width=80))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "llama_cpp_canister",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
