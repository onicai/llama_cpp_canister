{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI agent prompt design for llama_cpp_canister"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify we're in the Conda environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import base64\n",
    "import io\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import pprint\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import jupyter_black\n",
    "import textwrap\n",
    "\n",
    "from run_llama_cpp import run_llama_cpp\n",
    "\n",
    "# Activate the jupyter_black extension, which reformats code cells with black\n",
    "# https://github.com/n8henrie/jupyter-black\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before running notebook, build regular llama.cpp\n",
    "\n",
    "To build llama.cpp, follow the instructions in the README at:\n",
    "\n",
    "https://github.com/ggml-org/llama.cpp\n",
    "\n",
    "```bash\n",
    "cd ../../  # sibling directory to llama_cpp_canister\n",
    "git clone git@github.com:ggml-org/llama.cpp.git ggml_org_llama_615212.cpp  \n",
    "cd ggml_org_llama_615212.cpp\n",
    "git checkout 615212\n",
    "cmake -B build\n",
    "cmake --build build --config Release -j 8\n",
    "```\n",
    "\n",
    "Then, define LLAMA_CLI_PATH as the location of `llama-cli`, relative to this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lama.cpp git sha 615212 is used by current version of llama_cpp_canister\n",
    "LLAMA_CLI_PATH = \"../../ggml_org_llama_615212.cpp/build/bin/llama-cli\"\n",
    "\n",
    "# lama.cpp git sha b841d0 was previous version used by llama_cpp_canister\n",
    "# LLAMA_CLI_PATH = \"../../ggml_org_llama_b841d0.cpp/llama-cli\"\n",
    "\n",
    "\n",
    "# ####################################################################### #\n",
    "# Select the MODEL_TYPE and MODEL (location is relative to this notebook) #\n",
    "# ####################################################################### #\n",
    "\n",
    "seed = 42\n",
    "num_tokens = 1024\n",
    "temp = 0.6\n",
    "# top_k = 50\n",
    "# top_p = 0.95\n",
    "# min_p = 0.05\n",
    "# tfs = 0.9\n",
    "# typical = 0.9\n",
    "# mirostat = 2\n",
    "# mirostat_lr = 0.1\n",
    "# mirostat_ent = 5.0\n",
    "repeat_penalty = 1.1\n",
    "\n",
    "# Notes:\n",
    "#                                     <not quantized>|<         quantized                >\n",
    "#  --cache-type-k has allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
    "#  --cache-type-v is not tested because that requires a GPU,\n",
    "#                 which is not available right now in an Internet Computer canister\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# 135 Million parameters\n",
    "\n",
    "# https://huggingface.co/tensorblock/SmolLM2-135M-Instruct-GGUF\n",
    "# MODEL_TYPE = \"SmolLM2\"\n",
    "# MODEL=\"../models/tensorblock/SmolLM2-135M-Instruct-GGUF/SmolLM2-135M-Instruct-Q8_0.gguf\"\n",
    "# cache_type_k = \"f16\"\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# 163 Million parameters\n",
    "\n",
    "# https://huggingface.co/tensorblock/gpt2-GGUF (124M)\n",
    "# MODEL_TYPE = \"gpt2\"\n",
    "# MODEL = \"../models/tensorblock/gpt2-GGUF/gpt2-Q8_0.gguf\"\n",
    "# cache_type_k = \"f16\"\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# 630 Million parameters\n",
    "\n",
    "# https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF\n",
    "# MODEL_TYPE = \"Qwen\"\n",
    "# MODEL = \"../models/Qwen/Qwen2.5-0.5B-Instruct-GGUF/qwen2.5-0.5b-instruct-q4_k_m.gguf\"\n",
    "# cache_type_k = \"f16\"\n",
    "\n",
    "MODEL_TYPE = \"Qwen\"\n",
    "MODEL = \"../models/Qwen/Qwen2.5-0.5B-Instruct-GGUF/qwen2.5-0.5b-instruct-q8_0.gguf\"\n",
    "cache_type_k = \"q8_0\"\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# 1.24 Billion parameters\n",
    "\n",
    "# https://huggingface.co/unsloth/Llama-3.2-1B-Instruct-GGUF\n",
    "# MODEL_TYPE = \"Llama-3.2\"\n",
    "# MODEL = \"../models/unsloth/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct-Q4_K_M.gguf\"\n",
    "# cache_type_k = \"q5_0\"\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# 1.78 Billion parameters\n",
    "\n",
    "# WORK-IN-PROGRESS...\n",
    "\n",
    "# https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF\n",
    "# MODEL_TYPE = \"Qwen\"\n",
    "# MODEL = \"../models/Qwen/Qwen2.5-1.5B-Instruct-GGUF/qwen2.5-1.5b-instruct-q4_k_m.gguf\"\n",
    "# cache_type_k = \"q8_0\"\n",
    "\n",
    "# MODEL_TYPE = \"Qwen\"\n",
    "# MODEL = \"../models/Qwen/Qwen2.5-1.5B-Instruct-GGUF/qwen2.5-1.5b-instruct-q8_0.gguf\"\n",
    "# cache_type_k = \"q8_0\"\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# 1.78 Billion parameters\n",
    "\n",
    "# WORK-IN-PROGRESS...\n",
    "\n",
    "# https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF\n",
    "# MODEL_TYPE = \"DeepSeek-R1-Distill-Qwen\"\n",
    "# MODEL = \"../models/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.gguf\"\n",
    "# cache_type_k = \"q5_0\"\n",
    "\n",
    "# MODEL = \"../models/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/DeepSeek-R1-Distill-Qwen-1.5B-Q2_K_L.gguf\"\n",
    "# cache_type_k = \"q5_0\"\n",
    "\n",
    "# MODEL = \"../models/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_M.gguf\"\n",
    "# cache_type_k = \"q5_0\"\n",
    "\n",
    "# MODEL = \"../models/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf\"\n",
    "# cache_type_k = \"q5_0\"\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# MODEL_TYPE = \"Llama-3.2\"\n",
    "\n",
    "# https://huggingface.co/unsloth/Llama-3.2-3B-Instruct-GGUF\n",
    "# MODEL = \"../models/unsloth/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct-Q2_K.gguf\"\n",
    "\n",
    "prompt = \"\"\n",
    "question = \"give me a short introduction to LLMs.\"\n",
    "# question = \"What is the Internet Computer Protocol?\"\n",
    "# question = \"What is a blockchain?\"\n",
    "# question = \"What is the term for a blockchain that operates in parallel with other blockchains, allowing for cross-chain transactions?\"\n",
    "# question = \"Who invented the telescope?\"\n",
    "# question = \"Where does a butterfly emerge from?\"\n",
    "# question = \"What is 1+1?\"\n",
    "# question = \"When is Bitcoin first released?\"\n",
    "if MODEL_TYPE == \"gpt2\":\n",
    "    prompt = f\"{question}.\"\n",
    "elif MODEL_TYPE == \"SmolLM2\":\n",
    "    prompt = f\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "elif MODEL_TYPE == \"Qwen\":\n",
    "    prompt = f\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "elif MODEL_TYPE == \"Llama-3.2\":\n",
    "    system_prompt = \"\"\n",
    "    prompt = f\"<|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "elif MODEL_TYPE == \"DeepSeek-R1-Distill-Qwen\":\n",
    "    \"\"\"\n",
    "    From: https://deepinfra.com/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\n",
    "\n",
    "    Usage Recommendations\n",
    "\n",
    "    We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models,\n",
    "    including benchmarking, to achieve the expected performance:\n",
    "\n",
    "    1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n",
    "    2. Avoid adding a system prompt; all instructions should be contained within the user prompt.\n",
    "    3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n",
    "    4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n",
    "    \"\"\"\n",
    "    prompt = f\"<｜User｜>{question}. <｜Assistant｜>\"\n",
    "else:\n",
    "    print(f\"Model type {MODEL_TYPE} not recognized\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"\\nprompt:\\n\", textwrap.fill(prompt, width=80))\n",
    "\n",
    "run_llama_cpp(\n",
    "    LLAMA_CLI_PATH,\n",
    "    MODEL,\n",
    "    prompt,\n",
    "    num_tokens,\n",
    "    seed,\n",
    "    temp,\n",
    "    # top_k,\n",
    "    # top_p,\n",
    "    # min_p,\n",
    "    # tfs,\n",
    "    # typical,\n",
    "    # mirostat,\n",
    "    # mirostat_lr,\n",
    "    # mirostat_ent,\n",
    "    repeat_penalty,\n",
    "    cache_type_k,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_cpp_canister",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
